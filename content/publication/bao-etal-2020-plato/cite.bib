@inproceedings{bao-etal-2020-plato,
 abstract = {Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering. In this framework, we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. We also introduce discrete latent variables to tackle the inherent one-to-many mapping problem in response generation. Two reciprocal tasks of response generation and latent act recognition are designed and carried out simultaneously within a shared network. Comprehensive experiments on three publicly available datasets verify the effectiveness and superiority of the proposed framework.},
 address = {Online},
 author = {Bao, Siqi  and
He, Huang  and
Wang, Fan  and
Wu, Hua  and
Wang, Haifeng},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/2020.acl-main.9},
 month = {July},
 pages = {85--96},
 publisher = {Association for Computational Linguistics},
 title = {PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable},
 url = {https://www.aclweb.org/anthology/2020.acl-main.9},
 year = {2020}
}

